import BP_learn
import numpy as np

def test_num_of_hidden():
	node_number = [16, 0, 3]
	scope = '-1-1'
	learn_rate = 0.5
	activation = 'bipolar'
	# 三组训练数据
	data = [
	[[1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, -1, -1, 1], [1, -1, -1]],
	[[-1, 1, -1, -1, -1, 1, -1, -1, -1, 1, -1, -1, -1, 1, -1, -1], [-1, 1, -1]],
	[[1, 1, 1, 1, 1, -1, -1, 1, 1, -1, -1, 1, 1, 1, 1, 1], [-1, -1, 1]]
	]
	for i in range(4, 18, 1):
		average = 0
		for j in range(1):
			node_number[1] = i
			# print(node_number)
			v, w, ci, co = BP_learn.Initialize_weights(scope, node_number)
			# print(v)
			# print(w)
			# iterations = 50000
			k = 0
			config = None
			while True:
				error = 0.0
				for d in data:
					inputs = d[0]
					targets = d[1]
					v, w, b_h, y_i = BP_learn.feed_forward(node_number, activation, v, w, inputs)
					v, w, J, config = BP_learn.back_propagation(node_number, activation, v, w, ci, co, b_h, y_i, learn_rate, targets, inputs, config)
				
				if k % 10 == 0:
					filename = 'test_num_of_hidden' + str(i) + '.txt'
					with open(filename, 'a') as f:
						f.write('%d\t%.8f\n' % ((k + 1), J))
				
				# if k % 100 == 0:
				# 	print('第%d轮误差为%.8f' % (k, J))
				k += 1
				if J <= 0.00001:
					
					filename = 'test_num_of_hidden' + str(i) + '.txt'
					with open(filename, 'a') as f:
						f.write('%d\t%.8f\n' % ((k + 1), J))
					
					print('隐层数为%d时，在第%d轮达到误差要求' % (i, k))
					average += k 
					break
				if k >= 50000:
					print('未在规定次数中收敛')
					break
		# average = average / 10.0
		# 1print('%d个隐层的平均训练次数为%f' % (i, average))

def test_learn_rate():
	node_number = [16, 15, 3]
	scope = '-1-1'
	activation = 'bipolar'
	# 三组训练数据
	data = [
		[[1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, -1, -1, 1], [1, -1, -1]],
		[[-1, 1, -1, -1, -1, 1, -1, -1, -1, 1, -1, -1, -1, 1, -1, -1], [-1, 1, -1]],
		[[1, 1, 1, 1, 1, -1, -1, 1, 1, -1, -1, 1, 1, 1, 1, 1], [-1, -1, 1]]
	]
	# learn = [0.01, 0.08, 0.2, 0.5, 0.8]
	# learn = [0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24]
	learn = [0.20]
	for i in range(10):
		v, w, ci, co = BP_learn.Initialize_weights(scope, node_number)
		'''
		ci = np.zeros([node_number[0], node_number[1]])
		co = np.zeros([node_number[1], node_number[2]])
		v = [[ 0.07865436,  0.80012663, -0.14674322,  0.07507775,  0.75566393,
        	 0.02940462,  0.7919148 ,  0.81936534, -0.38153472, -0.6241032 ,
     	  	  0.0687816 ,  0.6558532 , -0.07801176, -0.36124712,  0.50221647],
     	 	 [ 0.90900149, -0.32368204,  0.18257443, -0.00224815,  0.9508085 ,
     	    0.85985976, -0.28242442, -0.70455756, -0.39126224,  0.44082616,
     	   -0.75463025, -0.20847431, -0.43367996,  0.65028112,  0.3469111 ],
     	  [ 0.03793304,  0.72790526,  0.15556716, -0.58968509, -0.38675777,
     	   -0.42848264, -0.07131575,  0.81969824,  0.15536607,  0.95409379,
     	   -0.7744922 ,  0.80078176,  0.23647938,  0.78982558, -0.59537746],
     	  [-0.81443784, -0.82321677,  0.04045097,  0.60098003,  0.68153769,
     	    0.92997275,  0.60635533, -0.97226996,  0.05114904, -0.04537407,
     	   -0.83781964,  0.10786614, -0.03325725,  0.22289747,  0.15960152],
     	  [-0.90361147,  0.10730392, -0.34164974,  0.59268355, -0.61261337,
     	   -0.02793003,  0.09083058, -0.76170922,  0.79633492, -0.46312523,
     	    0.95023363, -0.02752201, -0.4790516 ,  0.54971091,  0.10946646],
     	  [ 0.41799509,  0.49637958, -0.55837229, -0.86757647,  0.4310763 ,
     	   -0.46453407,  0.09019136, -0.66218415, -0.82102355,  0.0467041 ,
     	   -0.3497487 , -0.41921158,  0.45077326,  0.25942682,  0.95753737],
     	  [-0.46999499, -0.17959411, -0.88017804,  0.03505399,  0.63750398,
     	    0.6780259 ,  0.47612353, -0.70565075, -0.43962087,  0.26737214,
     	   -0.33272945, -0.80075395,  0.3280042 ,  0.55797651, -0.25319059],
     	  [ 0.21559789, -0.65935453,  0.04097062,  0.82234121,  0.72270475,
     	   -0.007057  , -0.04945217, -0.36801021, -0.15976985,  0.53685574,
     	   -0.12818376, -0.51062634, -0.52601419,  0.48951523, -0.16534072],
     	  [ 0.20451836,  0.68314283, -0.56695888,  0.89239332, -0.46740214,
     	    0.9545365 , -0.26238555,  0.47983799, -0.89647241, -0.19045287,
     	   -0.01996989,  0.93639298, -0.97142913,  0.58594757,  0.62134198],
     	  [ 0.63148584,  0.93881285,  0.46285269,  0.86787892, -0.28715984,
     	   -0.97416231, -0.6969705 , -0.00474909,  0.24139342,  0.0405196 ,
     	    0.92794636, -0.04043281,  0.90832342, -0.38359561,  0.06189485],
     	  [-0.57476709,  0.94004045, -0.66648288, -0.50116717, -0.04757168,
     	    0.80472256,  0.30580618,  0.78793064,  0.15337504, -0.42153306,
     	    0.62081964,  0.67199699,  0.79948366,  0.19194477,  0.88577321],
     	  [ 0.91579048,  0.38200662, -0.86724508,  0.82865566, -0.78770446,
     	    0.51294818,  0.90487492,  0.39076459,  0.63597411,  0.28030484,
     	   -0.36127777, -0.77706899,  0.01642042, -0.86068138, -0.69326525],
     	  [ 0.43008948, -0.73819218,  0.90217626, -0.15264191,  0.25629264,
     	    0.96082401,  0.68427334,  0.63303673,  0.80049807, -0.60121287,
     	    0.87071748,  0.27375488,  0.79799599, -0.6180923 ,  0.01177856],
     	  [ 0.07459346,  0.79684757,  0.30422922,  0.97160797,  0.62578933,
     	   -0.11625144,  0.96070562,  0.61326926, -0.51768979,  0.27575258,
     	   -0.53216049,  0.89355675, -0.32120903,  0.2172648 , -0.96999964],
     	  [-0.14872968, -0.16844562,  0.92892651, -0.36629812,  0.0943005 ,
     	    0.25322861,  0.90752391, -0.30708772,  0.27949984, -0.77800511,
     	    0.85122461, -0.42903833, -0.11265057,  0.24552633, -0.77785551],
     	  [ 0.68101486, -0.16703834, -0.64956013,  0.75328198, -0.78526328,
     	   -0.53146632, -0.18657289, -0.32594343,  0.15422796,  0.32094638,
     	    0.99926858, -0.85031511, -0.04408722, -0.81872082,  0.92603812]]
		w = [[ 0.590753  ,  0.39978469,  0.03093528],
     	  [-0.18601737,  0.39018917, -0.58096191],
     	  [ 0.12087574, -0.73179556, -0.65430058],
     	  [ 0.35164123,  0.43362053,  0.41221499],
     	  [-0.45577619, -0.64470048, -0.44950982],
     	  [-0.75190379,  0.31239616, -0.64437483],
     	  [-0.67526635,  0.78477343, -0.86954878],
     	  [ 0.41755216, -0.46083089, -0.21043646],
     	  [-0.95929756, -0.37938586,  0.68202905],
     	  [ 0.76080425,  0.67516274, -0.39580073],
     	  [-0.4851413 , -0.59822219,  0.33915406],
     	  [-0.03372722,  0.60517778, -0.98441704],
     	  [-0.62537699, -0.40476764,  0.9251077 ],
     	  [ 0.66002226,  0.83616518,  0.35711335],
     	  [-0.33583422,  0.49275488,  0.21466206]]
     	'''
		# learn_rate = learn[i]
		learn_rate = learn[0]
		k = 0
		config = None
		while True:
			error = 0.0
			for d in data:
				inputs = d[0]
				targets = d[1]
				v, w, b_h, y_i = BP_learn.feed_forward(node_number, activation, v, w, inputs)
				v, w, J, config = BP_learn.back_propagation(node_number, activation, v, w, ci, co, b_h, y_i, learn_rate, targets, inputs, config)
			# 计算每次的结果并写入到文件中进行保存
			'''	
			if k % 10 == 0:
				filename = 'test_learn_rate' + str(learn[i]) + '.txt'
				with open(filename, 'a') as f:
					f.write('%d\t%.8f\n' % ((k + 1), J))
			'''
			if J <= 0.00001:
				# filename = 'test_learn_rate' + str(learn_rate) + '.txt'
				# with open(filename, 'a') as f:
				# 	f.write('%d\t%.8f\n' % ((k + 1), J))
				print('学习率为%f，在第%d轮达到误差要求' % (learn_rate, k))
				break
			
			if k >= 50000:
				print('未在规定次数中收敛')
				break
			k += 1

def test_adaptive_learn_rate(p):
	node_number = [16, 15, 3]
	scope = '-1-1'
	activation = 'bipolar'
	# 三组训练数据
	data = [
		[[1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, -1, -1, 1], [1, -1, -1]],
		[[-1, 1, -1, -1, -1, 1, -1, -1, -1, 1, -1, -1, -1, 1, -1, -1], [-1, 1, -1]],
		[[1, 1, 1, 1, 1, -1, -1, 1, 1, -1, -1, 1, 1, 1, 1, 1], [-1, -1, 1]]
	]
	learn_rate = 0.2
	# v, w, ci, co = BP_learn.Initialize_weights(scope, node_number)
	
	ci = np.zeros([node_number[0], node_number[1]])
	co = np.zeros([node_number[1], node_number[2]])
	v = [[ 0.07865436,  0.80012663, -0.14674322,  0.07507775,  0.75566393,
        	 0.02940462,  0.7919148 ,  0.81936534, -0.38153472, -0.6241032 ,
     	  	  0.0687816 ,  0.6558532 , -0.07801176, -0.36124712,  0.50221647],
     	 	 [ 0.90900149, -0.32368204,  0.18257443, -0.00224815,  0.9508085 ,
     	    0.85985976, -0.28242442, -0.70455756, -0.39126224,  0.44082616,
     	   -0.75463025, -0.20847431, -0.43367996,  0.65028112,  0.3469111 ],
     	  [ 0.03793304,  0.72790526,  0.15556716, -0.58968509, -0.38675777,
     	   -0.42848264, -0.07131575,  0.81969824,  0.15536607,  0.95409379,
     	   -0.7744922 ,  0.80078176,  0.23647938,  0.78982558, -0.59537746],
     	  [-0.81443784, -0.82321677,  0.04045097,  0.60098003,  0.68153769,
     	    0.92997275,  0.60635533, -0.97226996,  0.05114904, -0.04537407,
     	   -0.83781964,  0.10786614, -0.03325725,  0.22289747,  0.15960152],
     	  [-0.90361147,  0.10730392, -0.34164974,  0.59268355, -0.61261337,
     	   -0.02793003,  0.09083058, -0.76170922,  0.79633492, -0.46312523,
     	    0.95023363, -0.02752201, -0.4790516 ,  0.54971091,  0.10946646],
     	  [ 0.41799509,  0.49637958, -0.55837229, -0.86757647,  0.4310763 ,
     	   -0.46453407,  0.09019136, -0.66218415, -0.82102355,  0.0467041 ,
     	   -0.3497487 , -0.41921158,  0.45077326,  0.25942682,  0.95753737],
     	  [-0.46999499, -0.17959411, -0.88017804,  0.03505399,  0.63750398,
     	    0.6780259 ,  0.47612353, -0.70565075, -0.43962087,  0.26737214,
     	   -0.33272945, -0.80075395,  0.3280042 ,  0.55797651, -0.25319059],
     	  [ 0.21559789, -0.65935453,  0.04097062,  0.82234121,  0.72270475,
     	   -0.007057  , -0.04945217, -0.36801021, -0.15976985,  0.53685574,
     	   -0.12818376, -0.51062634, -0.52601419,  0.48951523, -0.16534072],
     	  [ 0.20451836,  0.68314283, -0.56695888,  0.89239332, -0.46740214,
     	    0.9545365 , -0.26238555,  0.47983799, -0.89647241, -0.19045287,
     	   -0.01996989,  0.93639298, -0.97142913,  0.58594757,  0.62134198],
     	  [ 0.63148584,  0.93881285,  0.46285269,  0.86787892, -0.28715984,
     	   -0.97416231, -0.6969705 , -0.00474909,  0.24139342,  0.0405196 ,
     	    0.92794636, -0.04043281,  0.90832342, -0.38359561,  0.06189485],
     	  [-0.57476709,  0.94004045, -0.66648288, -0.50116717, -0.04757168,
     	    0.80472256,  0.30580618,  0.78793064,  0.15337504, -0.42153306,
     	    0.62081964,  0.67199699,  0.79948366,  0.19194477,  0.88577321],
     	  [ 0.91579048,  0.38200662, -0.86724508,  0.82865566, -0.78770446,
     	    0.51294818,  0.90487492,  0.39076459,  0.63597411,  0.28030484,
     	   -0.36127777, -0.77706899,  0.01642042, -0.86068138, -0.69326525],
     	  [ 0.43008948, -0.73819218,  0.90217626, -0.15264191,  0.25629264,
     	    0.96082401,  0.68427334,  0.63303673,  0.80049807, -0.60121287,
     	    0.87071748,  0.27375488,  0.79799599, -0.6180923 ,  0.01177856],
     	  [ 0.07459346,  0.79684757,  0.30422922,  0.97160797,  0.62578933,
     	   -0.11625144,  0.96070562,  0.61326926, -0.51768979,  0.27575258,
     	   -0.53216049,  0.89355675, -0.32120903,  0.2172648 , -0.96999964],
     	  [-0.14872968, -0.16844562,  0.92892651, -0.36629812,  0.0943005 ,
     	    0.25322861,  0.90752391, -0.30708772,  0.27949984, -0.77800511,
     	    0.85122461, -0.42903833, -0.11265057,  0.24552633, -0.77785551],
     	  [ 0.68101486, -0.16703834, -0.64956013,  0.75328198, -0.78526328,
     	   -0.53146632, -0.18657289, -0.32594343,  0.15422796,  0.32094638,
     	    0.99926858, -0.85031511, -0.04408722, -0.81872082,  0.92603812]]
	w = [[ 0.590753  ,  0.39978469,  0.03093528],
     	  [-0.18601737,  0.39018917, -0.58096191],
     	  [ 0.12087574, -0.73179556, -0.65430058],
     	  [ 0.35164123,  0.43362053,  0.41221499],
     	  [-0.45577619, -0.64470048, -0.44950982],
     	  [-0.75190379,  0.31239616, -0.64437483],
     	  [-0.67526635,  0.78477343, -0.86954878],
     	  [ 0.41755216, -0.46083089, -0.21043646],
     	  [-0.95929756, -0.37938586,  0.68202905],
     	  [ 0.76080425,  0.67516274, -0.39580073],
     	  [-0.4851413 , -0.59822219,  0.33915406],
     	  [-0.03372722,  0.60517778, -0.98441704],
     	  [-0.62537699, -0.40476764,  0.9251077 ],
     	  [ 0.66002226,  0.83616518,  0.35711335],
     	  [-0.33583422,  0.49275488,  0.21466206]]
    
	k = 0
	config = None
	while True:
		error = 0.0
		for d in data:
			inputs = d[0]
			targets = d[1]
			v, w, b_h, y_i = BP_learn.feed_forward(node_number, activation, v, w, inputs)
			v, w, J, config = BP_learn.back_propagation(node_number, activation, v, w, ci, co, b_h, y_i, learn_rate, targets, inputs, config, Ada = True)
		# 计算每次的结果并写入到文件中进行保存
		
		if k % 1 == 0:
			filename = 'test_learn_rate_ada.txt'
			with open(filename, 'a') as f:
				f.write('%d\t%.8f\n' % ((k + 1), J))
			print('第%d轮误差为%.8f' % (k, J))
		if J <= 0.00001:
			filename = 'test_learn_rate_ada.txt'
			with open(filename, 'a') as f:
				f.write('%d\t%.8f\n' % ((k + 1), J))
			print('学习率为%f，在第%d轮达到误差要求' % (learn_rate, k))
			break
		
		if k >= 50000:
			print('未在规定次数中收敛')
			break
		k += 1
	


if __name__=='__main__':
	# test_num_of_hidden()
	# test_learn_rate()
	# for i in range(10):
	test_adaptive_learn_rate(1)